name: tm-lambda
services:
  zookeeper:
    image: bitnami/zookeeper:3.9
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    networks: [bigdata]

  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    depends_on: [zookeeper]
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,PLAINTEXT_HOST://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_ENABLE_KRAFT=no
      - ALLOW_PLAINTEXT_LISTENER=yes
    ports:
      - "29092:29092"
    networks: [bigdata]

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on: [kafka]
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
    ports:
      - "8088:8080"
    networks: [bigdata]

  # HDFS (images amd64 -> on force la plateforme sur Mac M1/M2)
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    platform: linux/amd64
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=tm-hdfs
    volumes:
      - ./data/hdfs/namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
    networks: [bigdata]

  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    platform: linux/amd64
    container_name: hdfs-datanode
    depends_on: [hdfs-namenode]
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
    volumes:
      - ./data/hdfs/datanode:/hadoop/dfs/data
    ports:
      - "9864:9864"
    networks: [bigdata]

  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./exports:/mnt/exports
    networks: [bigdata]

  spark-worker-1:
    image: bitnami/spark:3.5
    container_name: spark-worker-1
    depends_on: [spark-master]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
    ports:
      - "8081:8081"
    volumes:
      - ./exports:/mnt/exports
    networks: [bigdata]

  airflow-postgres:
    image: postgres:16
    container_name: airflow-postgres
    environment:
      - POSTGRES_USER=${AIRFLOW_DB_USER}
      - POSTGRES_PASSWORD=${AIRFLOW_DB_PASSWORD}
      - POSTGRES_DB=${AIRFLOW_DB_NAME}
    volumes:
      - ./data/postgres-airflow:/var/lib/postgresql/data
    networks: [bigdata]

  # One-shot : init/migrate DB + création admin (tout en UNE seule ligne)
  airflow-init:
    image: ${AIRFLOW_IMAGE_NAME}
    container_name: airflow-init
    depends_on: [airflow-postgres]
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW_UID=50000
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: /bin/bash
    command: >
      -c "airflow db migrate &&
          airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin || true"
    networks: [bigdata]

  # Webserver + Scheduler (DB déjà initialisée)
  airflow:
    image: ${AIRFLOW_IMAGE_NAME}
    container_name: airflow
    depends_on: [airflow-postgres]
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY}
      - AIRFLOW_UID=50000
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: /bin/bash
    command: -c "airflow webserver & airflow scheduler"
    ports:
      - "8089:8080"
    networks: [bigdata]

networks:
  bigdata:
    driver: bridge
